{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Batch processing with Azure pipelines\n",
    "Azure Machine Learning pipelines can either be created in the designer or with the python azureml API.\n",
    "In this lab we are going to create a simple Azure pipeline for batch processing. The pipeline consists of two steps- preprocessing and scoring.\n",
    "Be aware that we are going to use experimental features of azureml which should not be used in a productive environment.\n",
    "Lets first import all needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "from azureml.data.output_dataset_config import OutputFileDatasetConfig\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import RunConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Connect to workspace, set up dataset and compute\n",
    "To have a more realistic setting we are not going to use our registered dataset, but the csv file with the raw credit data directly. Be aware, with this setting we are using our training data for prediction. This is just feasible for demonstration purpose, it is not something you would want to do in production. We create a DatasetConsumptionConfig for data input at the beginning of the pipeline. Two OutputFileDatasetConfig objects serve as intermediate and final location for the output files. The result_data will be registered as new dataset (batch-scoring-results) which is accomplished with the command register_on_complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Class OutputFileDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "WARNING - Class OutputDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "WARNING - Class OutputFileDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "WARNING - Class OutputDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "WARNING - Class RegistrationConfiguration: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "dataset = Dataset.Tabular.from_delimited_files(path=[(datastore, 'german_credit_dataset.csv')])\n",
    "input_data = DatasetConsumptionConfig(\"input_dataset\", dataset)\n",
    "intermediate_data = OutputFileDatasetConfig(name='intermediate_dataset', destination=(datastore, 'intermediate/{run-id}'))\n",
    "result_data = OutputFileDatasetConfig(name='result_dataset', destination=(datastore, 'result/{run-id}')).register_on_complete('batch-scoring-results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the compute \"batch-comp\" is not available in your workspace, it will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_name = 'batch-comp'\n",
    "\n",
    "# checks to see if compute target already exists in workspace, else create it\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
    "else:\n",
    "    config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_DS11_V2\",\n",
    "                                                   vm_priority=\"lowpriority\",\n",
    "                                                   min_nodes=1,\n",
    "                                                   max_nodes=2)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace=ws, name=compute_name, provisioning_configuration=config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A run configuration based on the conda dependencies is automatically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"script\": \"train.py\",\n",
       "    \"arguments\": [],\n",
       "    \"target\": \"local\",\n",
       "    \"framework\": \"Python\",\n",
       "    \"communicator\": \"None\",\n",
       "    \"maxRunDurationSeconds\": null,\n",
       "    \"nodeCount\": 1,\n",
       "    \"environment\": {\n",
       "        \"name\": null,\n",
       "        \"version\": null,\n",
       "        \"environmentVariables\": {\n",
       "            \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "        },\n",
       "        \"python\": {\n",
       "            \"userManagedDependencies\": false,\n",
       "            \"interpreterPath\": \"python\",\n",
       "            \"condaDependenciesFile\": null,\n",
       "            \"baseCondaEnvironment\": null,\n",
       "            \"condaDependencies\": {\n",
       "                \"name\": \"project_environment\",\n",
       "                \"dependencies\": [\n",
       "                    \"python=3.6.2\",\n",
       "                    {\n",
       "                        \"pip\": [\n",
       "                            \"azureml-defaults\",\n",
       "                            \"scikit-learn==0.22\"\n",
       "                        ]\n",
       "                    }\n",
       "                ],\n",
       "                \"channels\": [\n",
       "                    \"anaconda\",\n",
       "                    \"conda-forge\"\n",
       "                ]\n",
       "            }\n",
       "        },\n",
       "        \"docker\": {\n",
       "            \"enabled\": false,\n",
       "            \"baseImage\": \"mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20200821.v1\",\n",
       "            \"baseDockerfile\": null,\n",
       "            \"sharedVolumes\": true,\n",
       "            \"shmSize\": \"2g\",\n",
       "            \"arguments\": [],\n",
       "            \"baseImageRegistry\": {\n",
       "                \"address\": null,\n",
       "                \"username\": null,\n",
       "                \"password\": null,\n",
       "                \"registryIdentity\": null\n",
       "            },\n",
       "            \"platform\": {\n",
       "                \"os\": \"Linux\",\n",
       "                \"architecture\": \"amd64\"\n",
       "            }\n",
       "        },\n",
       "        \"spark\": {\n",
       "            \"repositories\": [],\n",
       "            \"packages\": [],\n",
       "            \"precachePackages\": true\n",
       "        },\n",
       "        \"databricks\": {\n",
       "            \"mavenLibraries\": [],\n",
       "            \"pypiLibraries\": [],\n",
       "            \"rcranLibraries\": [],\n",
       "            \"jarLibraries\": [],\n",
       "            \"eggLibraries\": []\n",
       "        },\n",
       "        \"r\": null,\n",
       "        \"inferencingStackVersion\": null\n",
       "    },\n",
       "    \"history\": {\n",
       "        \"outputCollection\": true,\n",
       "        \"snapshotProject\": true,\n",
       "        \"directoriesToWatch\": [\n",
       "            \"logs\"\n",
       "        ]\n",
       "    },\n",
       "    \"spark\": {\n",
       "        \"configuration\": {\n",
       "            \"spark.app.name\": \"Azure ML Experiment\",\n",
       "            \"spark.yarn.maxAppAttempts\": 1\n",
       "        }\n",
       "    },\n",
       "    \"hdi\": {\n",
       "        \"yarnDeployMode\": \"cluster\"\n",
       "    },\n",
       "    \"tensorflow\": {\n",
       "        \"workerCount\": 1,\n",
       "        \"parameterServerCount\": 1\n",
       "    },\n",
       "    \"mpi\": {\n",
       "        \"processCountPerNode\": 1,\n",
       "        \"nodeCount\": 1\n",
       "    },\n",
       "    \"paralleltask\": {\n",
       "        \"maxRetriesPerWorker\": 0,\n",
       "        \"workerCountPerNode\": 1,\n",
       "        \"terminalExitCodes\": null\n",
       "    },\n",
       "    \"dataReferences\": {},\n",
       "    \"data\": {},\n",
       "    \"outputData\": {},\n",
       "    \"sourceDirectoryDataStore\": null,\n",
       "    \"amlcompute\": {\n",
       "        \"vmSize\": null,\n",
       "        \"vmPriority\": null,\n",
       "        \"retainCluster\": false,\n",
       "        \"name\": null,\n",
       "        \"clusterMaxNodeCount\": null\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_pip_package(\"scikit-learn==0.22\")\n",
    "config = RunConfiguration(conda_dependencies=conda_dep)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the pipeline steps\n",
    "We create two PythonScriptStep objects. For each object we need to supply a python script. The scripts are prepared in the batch_script folder and we load them only to have a look at it. You can find different pipeline steps [here](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import pickle\n",
      "import joblib\n",
      "import argparse\n",
      "from azureml.core.run import Run\n",
      "from azureml.core.model import Model\n",
      "\n",
      "# load data\n",
      "run = Run.get_context()\n",
      "data = run.input_datasets['input_dataset'].to_pandas_dataframe()\n",
      "\n",
      "# load model\n",
      "ws = run.experiment.workspace\n",
      "# model = Model(ws, 'german-credit-local-model').download(exist_ok=True)\n",
      "pipeline_path = Model.get_model_path('german-credit-local-model')\n",
      "pipeline = joblib.load(pipeline_path)\n",
      "\n",
      "# preprocess\n",
      "data.drop(\"Sno\", axis=1, inplace=True)\n",
      "X_raw = data.drop('Risk', axis=1)\n",
      "out = pipeline['preprocessor'].transform(X_raw)\n",
      "\n",
      "# save intermediate file\n",
      "parser = argparse.ArgumentParser(\"preprocess\")\n",
      "parser.add_argument(\"--intermediate-data-path\", type=str)\n",
      "args = parser.parse_args()\n",
      "if args.intermediate_data_path is not None:\n",
      "    os.makedirs(args.intermediate_data_path, exist_ok=True)\n",
      "    print(f\"{args.intermediate_data_path} created\")\n",
      "pickle.dump(out, open(f\"{args.intermediate_data_path}/preprocessed_features.pkl\", \"wb\" ) )\n",
      "data.to_csv(f'{args.intermediate_data_path}/preprocessed_data.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "with open(\"batch_scripts/preprocessing_step.py\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import pickle\n",
      "import joblib\n",
      "import argparse\n",
      "import pandas as pd\n",
      "from azureml.core.run import Run\n",
      "from azureml.core.model import Model\n",
      "\n",
      "# load arguments\n",
      "parser = argparse.ArgumentParser(\"preprocess\")\n",
      "parser.add_argument(\"--intermediate-data-path\", type=str)\n",
      "parser.add_argument(\"--result-data-path\", type=str)\n",
      "args = parser.parse_args()\n",
      "\n",
      "# load data\n",
      "run = Run.get_context()\n",
      "print(args.intermediate_data_path)\n",
      "features = pickle.load(open(f'{args.intermediate_data_path}/preprocessed_features.pkl', \"rb\"))\n",
      "data = pd.read_csv(f'{args.intermediate_data_path}/preprocessed_data.csv')\n",
      "\n",
      "# load model\n",
      "ws = run.experiment.workspace\n",
      "# model = Model(ws, 'german-credit-local-model').download(exist_ok=True)\n",
      "pipeline_path = Model.get_model_path('german-credit-local-model')\n",
      "pipeline = joblib.load(pipeline_path)\n",
      "\n",
      "# score\n",
      "out = pipeline['classifier'].predict(features)\n",
      "\n",
      "# save result file\n",
      "\n",
      "if args.result_data_path is not None:\n",
      "    os.makedirs(args.result_data_path, exist_ok=True)\n",
      "    print(f\"{args.result_data_path} created\")\n",
      "data['prediction'] = out\n",
      "data.to_csv(f'{args.result_data_path}/result_data.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "with open(\"batch_scripts/scoring_step.py\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two scripts, together with the locations and compute are given as inputs to the PythonScriptStep constructors. The allow_reuse flag will allow us to use the intermediate results from earlier runs, if there are any and the pipeline step has not changed since the last run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_step = PythonScriptStep(\n",
    "    script_name=\"preprocessing_step.py\",\n",
    "    name='preprocessing_step',\n",
    "    arguments=['--intermediate-data-path', intermediate_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=config,\n",
    "    inputs=[input_data],\n",
    "    outputs=[intermediate_data],\n",
    "    source_directory='./batch_scripts',\n",
    "    allow_reuse=True\n",
    ")\n",
    "scoring_step = PythonScriptStep(\n",
    "    script_name=\"scoring_step.py\",\n",
    "    name='scoring_step',\n",
    "    arguments=['--intermediate-data-path', intermediate_data, '--result-data-path', result_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=config,\n",
    "    inputs=[intermediate_data],\n",
    "    outputs=[result_data],\n",
    "    source_directory='./batch_scripts'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline\n",
    "We can combine the steps to a whole pipeline and submit the pipeline as a new experiment run. You can find all logs in your workspace. The intermediate and final file locations and data can be found your Azure blob storage which was created automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step preprocessing_step [c6418e91][7844aa30-ce1b-4ceb-801a-67f52d2f27a3], (This step will run and generate new outputs)\n",
      "Created step scoring_step [848b1189][0decd8a9-c3f6-4294-8a82-157f3df0682c], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun cc0f09ef-e18a-4e66-8eeb-32cef1630041\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/batch-score/runs/cc0f09ef-e18a-4e66-8eeb-32cef1630041?wsid=/subscriptions/823af982-da0d-47e1-8124-3c00e4053556/resourcegroups/jrie_test/workspaces/aml_test\n",
      "PipelineRunId: cc0f09ef-e18a-4e66-8eeb-32cef1630041\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/batch-score/runs/cc0f09ef-e18a-4e66-8eeb-32cef1630041?wsid=/subscriptions/823af982-da0d-47e1-8124-3c00e4053556/resourcegroups/jrie_test/workspaces/aml_test\n",
      "{'runId': 'cc0f09ef-e18a-4e66-8eeb-32cef1630041', 'status': 'Completed', 'startTimeUtc': '2021-01-18T15:41:49.074019Z', 'endTimeUtc': '2021-01-18T15:45:33.278511Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://amltest5537269948.blob.core.windows.net/azureml/ExperimentRun/dcid.cc0f09ef-e18a-4e66-8eeb-32cef1630041/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=1Wlb7uC9KS%2B8USw5r0EoYMzgtTAI6e5kYSu4DH9RNsc%3D&st=2021-01-18T15%3A35%3A35Z&se=2021-01-18T23%3A45%3A35Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://amltest5537269948.blob.core.windows.net/azureml/ExperimentRun/dcid.cc0f09ef-e18a-4e66-8eeb-32cef1630041/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=lFZAGW7sJMLPE4XdvR8UuOWagTg%2FUHkby1Ub%2FUyOlfY%3D&st=2021-01-18T15%3A35%3A35Z&se=2021-01-18T23%3A45%3A35Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://amltest5537269948.blob.core.windows.net/azureml/ExperimentRun/dcid.cc0f09ef-e18a-4e66-8eeb-32cef1630041/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=zxuA%2F0JQY2sPn7YgkolB2JttIzgCVb7Ap5G6sNCoISY%3D&st=2021-01-18T15%3A35%3A35Z&se=2021-01-18T23%3A45%3A35Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_pipeline = Pipeline(workspace=ws, steps=[preprocessing_step, scoring_step])\n",
    "pipeline_run = Experiment(ws, 'batch-score').submit(scoring_pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you are used from the designer, you can still monitor the pipeline during training in the experiments section (open the specific run) in your workspace\n",
    "<img src=\"images/pipeline-steps.jpg\" alt=\"Pipeline\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Let us have a look at the resulting data. We can easily access the results from the registered dataset. The result was automatically registered as batch-scoring-results as defined at the output location creation above. For comparison we open the original credit risk set, that we have registered in lab 3. We can see the added column \"prediction\". Of course, in a real-life scenario, you would not have the \"Risk\" column i.e. unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Job</th>\n",
       "      <th>Housing</th>\n",
       "      <th>Saving accounts</th>\n",
       "      <th>Checking account</th>\n",
       "      <th>Credit amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Risk</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>NaN</td>\n",
       "      <td>little</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>moderate</td>\n",
       "      <td>5951</td>\n",
       "      <td>48</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2096</td>\n",
       "      <td>12</td>\n",
       "      <td>education</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>7882</td>\n",
       "      <td>42</td>\n",
       "      <td>furniture/equipment</td>\n",
       "      <td>good</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>4870</td>\n",
       "      <td>24</td>\n",
       "      <td>car</td>\n",
       "      <td>bad</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age     Sex  Job Housing Saving accounts Checking account  Credit amount  \\\n",
       "0   67    male    2     own             NaN           little           1169   \n",
       "1   22  female    2     own          little         moderate           5951   \n",
       "2   49    male    1     own          little              NaN           2096   \n",
       "3   45    male    2    free          little           little           7882   \n",
       "4   53    male    2    free          little           little           4870   \n",
       "\n",
       "   Duration              Purpose  Risk prediction  \n",
       "0         6             radio/TV  good       good  \n",
       "1        48             radio/TV   bad        bad  \n",
       "2        12            education  good       good  \n",
       "3        42  furniture/equipment  good        bad  \n",
       "4        24                  car   bad       good  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.get_by_name(ws, name='batch-scoring-results', version = \"latest\")\n",
    "df_path = dataset.download('data/batch_scoring_results', overwrite=True)\n",
    "pd.read_csv(df_path[0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sno</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Job</th>\n",
       "      <th>Housing</th>\n",
       "      <th>Saving accounts</th>\n",
       "      <th>Checking account</th>\n",
       "      <th>Credit amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>NA</td>\n",
       "      <td>little</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>moderate</td>\n",
       "      <td>5951</td>\n",
       "      <td>48</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>NA</td>\n",
       "      <td>2096</td>\n",
       "      <td>12</td>\n",
       "      <td>education</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>7882</td>\n",
       "      <td>42</td>\n",
       "      <td>furniture/equipment</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>4870</td>\n",
       "      <td>24</td>\n",
       "      <td>car</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sno  Age     Sex  Job Housing Saving accounts Checking account  \\\n",
       "0    0   67    male    2     own              NA           little   \n",
       "1    1   22  female    2     own          little         moderate   \n",
       "2    2   49    male    1     own          little               NA   \n",
       "3    3   45    male    2    free          little           little   \n",
       "4    4   53    male    2    free          little           little   \n",
       "\n",
       "   Credit amount  Duration              Purpose  Risk  \n",
       "0           1169         6             radio/TV  good  \n",
       "1           5951        48             radio/TV   bad  \n",
       "2           2096        12            education  good  \n",
       "3           7882        42  furniture/equipment  good  \n",
       "4           4870        24                  car   bad  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.get_by_name(ws, name='german_credit_dataset', version = \"latest\")\n",
    "ds_df = dataset.to_pandas_dataframe()\n",
    "ds_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
