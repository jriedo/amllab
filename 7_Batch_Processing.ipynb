{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Batch processing with Azure pipelines\n",
    "Azure Machine Learning pipelines can either be created in the designer or with the python azureml API.\n",
    "In this lab we are going to create a simple Azure pipeline for batch processing. The pipeline consists of two steps- preprocessing and scoring.\n",
    "Be aware that we are going to use experimental features of azureml which should not be used in a productive environment.\n",
    "Lets first import all needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "from azureml.data.output_dataset_config import OutputFileDatasetConfig\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import RunConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Connect to workspace, set up dataset and compute\n",
    "To have a more realistic setting we are not going to use our registered dataset, but the csv file with the raw credit data directly. Be aware, with this setting we are using our training data for prediction. This is just feasible for demonstration purpose, it is not something you would want to do in production. We create a DatasetConsumptionConfig for data input at the beginning of the pipeline. Two OutputFileDatasetConfig objects serve as intermediate and final location for the output files. The result_data will be registered as new dataset (batch-scoring-results) which is accomplished with the command register_on_complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "dataset = Dataset.Tabular.from_delimited_files(path=[(datastore, 'german_credit_dataset.csv')])\n",
    "input_data = DatasetConsumptionConfig(\"input_dataset\", dataset)\n",
    "intermediate_data = OutputFileDatasetConfig(name='intermediate_dataset', destination=(datastore, 'intermediate/{run-id}'))\n",
    "result_data = OutputFileDatasetConfig(name='result_dataset', destination=(datastore, 'result/{run-id}')).register_on_complete('batch-scoring-results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the compute \"batch-comp\" is not available in your workspace, it will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_name = 'batch-comp'\n",
    "\n",
    "# checks to see if compute target already exists in workspace, else create it\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
    "else:\n",
    "    config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_DS11_V2\",\n",
    "                                                   vm_priority=\"lowpriority\",\n",
    "                                                   min_nodes=1,\n",
    "                                                   max_nodes=2)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace=ws, name=compute_name, provisioning_configuration=config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A run configuration based on the conda dependencies is automatically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_pip_package(\"scikit-learn==0.22\")\n",
    "config = RunConfiguration(conda_dependencies=conda_dep)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the pipeline steps\n",
    "We create two PythonScriptStep objects. For each object we need to supply a python script. The scripts are prepared in the batch_script folder and we load them only to have a look at it. You can find different pipeline steps [here](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"batch_scripts/preprocessing_step.py\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"batch_scripts/scoring_step.py\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two scripts, together with the locations and compute are given as inputs to the PythonScriptStep constructors. The allow_reuse flag will allow us to use the intermediate results from earlier runs, if there are any and the pipeline step has not changed since the last run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_step = PythonScriptStep(\n",
    "    script_name=\"preprocessing_step.py\",\n",
    "    name='preprocessing_step',\n",
    "    arguments=['--intermediate-data-path', intermediate_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=config,\n",
    "    inputs=[input_data],\n",
    "    outputs=[intermediate_data],\n",
    "    source_directory='./batch_scripts',\n",
    "    allow_reuse=True\n",
    ")\n",
    "scoring_step = PythonScriptStep(\n",
    "    script_name=\"scoring_step.py\",\n",
    "    name='scoring_step',\n",
    "    arguments=['--intermediate-data-path', intermediate_data, '--result-data-path', result_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=config,\n",
    "    inputs=[intermediate_data],\n",
    "    outputs=[result_data],\n",
    "    source_directory='./batch_scripts'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline\n",
    "We can combine the steps to a whole pipeline and submit the pipeline as a new experiment run. You can find all logs in your workspace. The intermediate and final file locations and data can be found your Azure blob storage which was created automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_pipeline = Pipeline(workspace=ws, steps=[preprocessing_step, scoring_step])\n",
    "pipeline_run = Experiment(ws, 'batch-score').submit(scoring_pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you are used from the designer, you can still monitor the pipeline during training in the experiments section (open the specific run) in your workspace\n",
    "<img src=\"images/pipeline-steps.jpg\" alt=\"Pipeline\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Let us have a look at the resulting data. We can easily access the results from the registered dataset. The result was automatically registered as batch-scoring-results as defined at the output location creation above. For comparison we open the original credit risk set, that we have registered in lab 3. We can see the added column \"prediction\". Of course, in a real-life scenario, you would not have the \"Risk\" column i.e. unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(ws, name='batch-scoring-results', version = \"latest\")\n",
    "df_path = dataset.download('data/batch_scoring_results', overwrite=True)\n",
    "pd.read_csv(df_path[0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(ws, name='german_credit_dataset', version = \"latest\")\n",
    "ds_df = dataset.to_pandas_dataframe()\n",
    "ds_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
